from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from dotenv import load_dotenv
import os
import logging

# ===============================
# Logging setup - Th√™m chi ti·∫øt h∆°n
# ===============================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ .env
load_dotenv(dotenv_path=os.path.join("/home/hadoop/project", ".env"))

# ===============================
# Config - Th√™m validation
# ===============================
KAFKA_CONFIG = {
    'bootstrap.servers': '192.168.235.136:9092,192.168.235.147:9092,192.168.235.148:9092',
}

# Ki·ªÉm tra c√°c bi·∫øn m√¥i tr∆∞·ªùng
required_env_vars = ['DB_HOST', 'DB_PORT', 'DB_NAME', 'DB_USER', 'DB_PASS']
for var in required_env_vars:
    if not os.getenv(var):
        raise ValueError(f"‚ùå Thi·∫øu bi·∫øn m√¥i tr∆∞·ªùng: {var}")

POSTGRES_CONFIG = {
    "url": f"jdbc:postgresql://{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}",
    "table": "banking_dw.fact_transaction",
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASS"),
    "driver": "org.postgresql.Driver"
}

# Log config ƒë·ªÉ debug
logger.info(f"üìä Database URL: {POSTGRES_CONFIG['url']}")
logger.info(f"üìä Database Table: {POSTGRES_CONFIG['table']}")

TRANSACTION_SCHEMA = StructType([
    StructField("account_key", IntegerType(), True),
    StructField("customer_key", IntegerType(), True),
    StructField("location_key", IntegerType(), True),
    StructField("event_key", LongType(), True),
    StructField("application_key", IntegerType(), True),
    StructField("transaction_id", StringType(), True),
    StructField("reference_number", StringType(), True),
    StructField("transaction_type", StringType(), True),
    StructField("transaction_category", StringType(), True),
    StructField("transaction_amount", DoubleType(), True),
    StructField("fee_amount", DoubleType(), True),
    StructField("tax_amount", DoubleType(), True),
    StructField("net_amount", DoubleType(), True),
    StructField("currency", StringType(), True),
    StructField("account_number", StringType(), True),
    StructField("transaction_status", StringType(), True),
    StructField("channel", StringType(), True),
    StructField("description", StringType(), True),
    StructField("created_timestamp", TimestampType(), True),
    StructField("processed_timestamp", TimestampType(), True),
    StructField("updated_timestamp", TimestampType(), True)
])

# ===============================
# Test k·∫øt n·ªëi database
# ===============================
def test_database_connection(spark):
    """Test k·∫øt n·ªëi database tr∆∞·ªõc khi streaming"""
    try:
        logger.info("üîç Ki·ªÉm tra k·∫øt n·ªëi database...")
        
        # T·∫°o DataFrame test ƒë∆°n gi·∫£n
        test_data = [(1, "test", 100.0)]
        test_schema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("amount", DoubleType(), True)
        ])
        
        test_df = spark.createDataFrame(test_data, test_schema)
        
        # Th·ª≠ ƒë·ªçc t·ª´ database (kh√¥ng c·∫ßn table t·ªìn t·∫°i)
        try:
            spark.read \
                .format("jdbc") \
                .option("url", POSTGRES_CONFIG["url"]) \
                .option("user", POSTGRES_CONFIG["user"]) \
                .option("password", POSTGRES_CONFIG["password"]) \
                .option("driver", POSTGRES_CONFIG["driver"]) \
                .option("query", "SELECT 1 as test") \
                .load() \
                .show()
            
            logger.info("‚úÖ K·∫øt n·ªëi database th√†nh c√¥ng!")
            return True
            
        except Exception as db_error:
            logger.error(f"‚ùå L·ªói k·∫øt n·ªëi database: {db_error}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå L·ªói test database: {e}")
        return False

# ===============================
# Spark Streaming class - C·∫£i thi·ªán
# ===============================
class SparkStreaming:
    def __init__(self, kafka_config, topics):
        self.spark = SparkSession.builder \
            .appName("KafkaSparkToPostgres") \
            .config("spark.sql.shuffle.partitions", "2") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .getOrCreate()

        self.spark.sparkContext.setLogLevel("WARN")
        self.kafka_config = kafka_config
        self.topics = topics
        
        # Test k·∫øt n·ªëi database
        if not test_database_connection(self.spark):
            raise Exception("Kh√¥ng th·ªÉ k·∫øt n·ªëi database")

    def read_stream(self):
        try:
            logger.info(f"üì° ƒêang k·∫øt n·ªëi Kafka: {self.kafka_config['bootstrap.servers']}")
            logger.info(f"üì° Topics: {self.topics}")
            
            df = self.spark.readStream \
                .format("kafka") \
                .option("kafka.bootstrap.servers", self.kafka_config['bootstrap.servers']) \
                .option("subscribe", ",".join(self.topics)) \
                .option("startingOffsets", "latest") \
                .option("failOnDataLoss", "false") \
                .option("maxOffsetsPerTrigger", "1000") \
                .load()

            # Th√™m debug ƒë·ªÉ xem raw data t·ª´ Kafka
            def debug_kafka_data(batch_df, batch_id):
                logger.info(f"üîç Debug Kafka Batch {batch_id}")
                logger.info(f"üìä Raw Kafka records: {batch_df.count()}")
                if batch_df.count() > 0:
                    batch_df.select("key", "value", "timestamp").show(5, truncate=False)

            # Debug stream (t·∫°m th·ªùi)
            debug_query = df.writeStream \
                .foreachBatch(debug_kafka_data) \
                .outputMode("append") \
                .option("checkpointLocation", "/tmp/debug_checkpoint") \
                .trigger(processingTime="30 seconds") \
                .start()

            # X·ª≠ l√Ω d·ªØ li·ªáu ch√≠nh
            processed_df = df.select(
                col("key").cast("string"),
                from_json(col("value").cast("string"), TRANSACTION_SCHEMA).alias("data"),
                col("timestamp").alias("kafka_timestamp")
            ).select("key", "data.*", "kafka_timestamp")

            logger.info("‚úÖ Kafka stream ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng")
            return processed_df

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o Kafka stream: {e}")
            raise

# ===============================
# Write batch to Postgres - C·∫£i thi·ªán
# ===============================
def write_to_postgres(batch_df, batch_id):
    try:
        logger.info(f"üîÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω Batch {batch_id}")
        
        # Debug: In schema v√† sample data
        logger.info(f"üìä Schema: {batch_df.schema}")
        
        row_count = batch_df.count()
        logger.info(f"üìä Batch {batch_id}: {row_count} records")
        
        if row_count > 0:
            # Hi·ªÉn th·ªã sample data
            logger.info("üìã Sample data:")
            batch_df.show(5, truncate=False)
            
            # Ki·ªÉm tra data quality
            valid_transactions = batch_df.filter(
                col("transaction_id").isNotNull() &
                col("transaction_amount").isNotNull() &
                (col("transaction_amount") > 0)
            )
            
            valid_count = valid_transactions.count()
            logger.info(f"üìä Valid records: {valid_count}/{row_count}")
            
            if valid_count > 0:
                # Th√™m timestamp
                final_df = valid_transactions.withColumn("created_at", current_timestamp())
                
                logger.info(f"üíæ ƒêang ghi {valid_count} records v√†o database...")
                
                final_df.write \
                    .format("jdbc") \
                    .option("url", POSTGRES_CONFIG["url"]) \
                    .option("dbtable", POSTGRES_CONFIG["table"]) \
                    .option("user", POSTGRES_CONFIG["user"]) \
                    .option("password", POSTGRES_CONFIG["password"]) \
                    .option("driver", POSTGRES_CONFIG["driver"]) \
                    .option("batchsize", "1000") \
                    .option("isolationLevel", "NONE") \
                    .option("numPartitions", "1") \
                    .mode("append") \
                    .save()

                logger.info(f"‚úÖ ƒê√£ ghi th√†nh c√¥ng batch {batch_id} - {valid_count} records")
            else:
                logger.warning(f"‚ö†Ô∏è Batch {batch_id}: Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá")
        else:
            logger.info(f"‚ö†Ô∏è Batch {batch_id} tr·ªëng, b·ªè qua")

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi ghi batch {batch_id}: {e}")
        # Log chi ti·∫øt l·ªói
        import traceback
        logger.error(f"‚ùå Stacktrace: {traceback.format_exc()}")

# ===============================
# Main - C·∫£i thi·ªán
# ===============================
def main():
    try:
        logger.info("üöÄ Kh·ªüi t·∫°o Spark Streaming...")
        
        # T·∫°o th∆∞ m·ª•c checkpoint n·∫øu ch∆∞a t·ªìn t·∫°i
        checkpoint_dir = "/user/hadoop/checkpoints/transaction_data"
        logger.info(f"üìÅ Checkpoint directory: {checkpoint_dir}")
        
        stream = SparkStreaming(KAFKA_CONFIG, ['transaction_data'])
        df = stream.read_stream()

        # Data quality checks
        df_filtered = df.filter(
            col("transaction_id").isNotNull() &
            col("transaction_amount").isNotNull() &
            (col("transaction_amount") > 0)
        )

        query = df_filtered.writeStream \
            .foreachBatch(write_to_postgres) \
            .outputMode("append") \
            .option("checkpointLocation", checkpoint_dir) \
            .trigger(processingTime="10 seconds") \
            .start()

        logger.info("‚úÖ Streaming ƒë√£ b·∫Øt ƒë·∫ßu. Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng...")
        
        # Monitor streaming
        while query.isActive:
            progress = query.lastProgress
            if progress:
                logger.info(f"üìä Streaming Progress: {progress}")
            query.awaitTermination(timeout=30)

    except KeyboardInterrupt:
        logger.info("üõë ƒêang d·ª´ng streaming...")
        if 'query' in locals():
            query.stop()
    except Exception as e:
        logger.error(f"‚ùå L·ªói trong main: {e}")
        import traceback
        logger.error(f"‚ùå Stacktrace: {traceback.format_exc()}")
        raise

if __name__ == "__main__":
    main()