from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from dotenv import load_dotenv
import os
import logging

# ===============================
# Logging setup - ThÃªm chi tiáº¿t hÆ¡n
# ===============================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load biáº¿n mÃ´i trÆ°á»ng tá»« .env
load_dotenv(dotenv_path=os.path.join("/home/hadoop/project", ".env"))

# ===============================
# Config - ThÃªm validation
# ===============================
KAFKA_CONFIG = {
    'bootstrap.servers': '192.168.235.136:9092,192.168.235.147:9092,192.168.235.148:9092',
}

# Kiá»ƒm tra cÃ¡c biáº¿n mÃ´i trÆ°á»ng
required_env_vars = ['DB_HOST', 'DB_PORT', 'DB_NAME', 'DB_USER', 'DB_PASS']
for var in required_env_vars:
    if not os.getenv(var):
        raise ValueError(f"âŒ Thiáº¿u biáº¿n mÃ´i trÆ°á»ng: {var}")

POSTGRES_CONFIG = {
    "url": f"jdbc:postgresql://{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}",
    "table": "banking_dw.fact_transaction",
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASS"),
    "driver": "org.postgresql.Driver"
}

# Log config Ä‘á»ƒ debug
logger.info(f"ğŸ“Š Database URL: {POSTGRES_CONFIG['url']}")
logger.info(f"ğŸ“Š Database Table: {POSTGRES_CONFIG['table']}")

TRANSACTION_SCHEMA = StructType([
    StructField("account_key", IntegerType(), True),
    StructField("customer_key", IntegerType(), True),
    StructField("location_key", IntegerType(), True),
    StructField("event_key", LongType(), True),
    StructField("application_key", IntegerType(), True),
    StructField("transaction_id", StringType(), True),
    StructField("reference_number", StringType(), True),
    StructField("transaction_type", StringType(), True),
    StructField("transaction_category", StringType(), True),
    StructField("transaction_amount", DoubleType(), True),
    StructField("fee_amount", DoubleType(), True),
    StructField("tax_amount", DoubleType(), True),
    StructField("net_amount", DoubleType(), True),
    StructField("currency", StringType(), True),
    StructField("account_number", StringType(), True),
    StructField("transaction_status", StringType(), True),
    StructField("channel", StringType(), True),
    StructField("description", StringType(), True),
    StructField("created_timestamp", TimestampType(), True),
    StructField("processed_timestamp", TimestampType(), True),
    StructField("updated_timestamp", TimestampType(), True)
])

# ===============================
# Test káº¿t ná»‘i database
# ===============================
def test_database_connection(spark):
    """Test káº¿t ná»‘i database trÆ°á»›c khi streaming"""
    try:
        logger.info("ğŸ” Kiá»ƒm tra káº¿t ná»‘i database...")
        
        # Táº¡o DataFrame test Ä‘Æ¡n giáº£n
        test_data = [(1, "test", 100.0)]
        test_schema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("amount", DoubleType(), True)
        ])
        
        test_df = spark.createDataFrame(test_data, test_schema)
        
        # Thá»­ Ä‘á»c tá»« database (khÃ´ng cáº§n table tá»“n táº¡i)
        try:
            spark.read \
                .format("jdbc") \
                .option("url", POSTGRES_CONFIG["url"]) \
                .option("user", POSTGRES_CONFIG["user"]) \
                .option("password", POSTGRES_CONFIG["password"]) \
                .option("driver", POSTGRES_CONFIG["driver"]) \
                .option("query", "SELECT 1 as test") \
                .load() \
                .show()
            
            logger.info("âœ… Káº¿t ná»‘i database thÃ nh cÃ´ng!")
            return True
            
        except Exception as db_error:
            logger.error(f"âŒ Lá»—i káº¿t ná»‘i database: {db_error}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Lá»—i test database: {e}")
        return False

# ===============================
# Spark Streaming class - Cáº£i thiá»‡n
# ===============================
class SparkStreaming:
    def __init__(self, kafka_config, topics):
        self.spark = SparkSession.builder \
            .appName("KafkaSparkToPostgres") \
            .config("spark.sql.shuffle.partitions", "2") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .getOrCreate()

        self.spark.sparkContext.setLogLevel("WARN")
        self.kafka_config = kafka_config
        self.topics = topics
        
        # Test káº¿t ná»‘i database
        if not test_database_connection(self.spark):
            raise Exception("KhÃ´ng thá»ƒ káº¿t ná»‘i database")

    def read_stream(self):
        try:
            logger.info(f"ğŸ“¡ Äang káº¿t ná»‘i Kafka: {self.kafka_config['bootstrap.servers']}")
            logger.info(f"ğŸ“¡ Topics: {self.topics}")
            
            df = self.spark.readStream \
                .format("kafka") \
                .option("kafka.bootstrap.servers", self.kafka_config['bootstrap.servers']) \
                .option("subscribe", ",".join(self.topics)) \
                .option("startingOffsets", "latest") \
                .option("failOnDataLoss", "false") \
                .option("maxOffsetsPerTrigger", "1000") \
                .load()

            # ThÃªm debug Ä‘á»ƒ xem raw data tá»« Kafka
            def debug_kafka_data(batch_df, batch_id):
                logger.info(f"ğŸ” Debug Kafka Batch {batch_id}")
                logger.info(f"ğŸ“Š Raw Kafka records: {batch_df.count()}")
                if batch_df.count() > 0:
                    batch_df.select("key", "value", "timestamp").show(5, truncate=False)

            # Debug stream (táº¡m thá»i)
            debug_query = df.writeStream \
                .foreachBatch(debug_kafka_data) \
                .outputMode("append") \
                .option("checkpointLocation", "/tmp/debug_checkpoint") \
                .trigger(processingTime="30 seconds") \
                .start()

            # Xá»­ lÃ½ dá»¯ liá»‡u chÃ­nh
            processed_df = df.select(
                col("key").cast("string"),
                from_json(col("value").cast("string"), TRANSACTION_SCHEMA).alias("data"),
                col("timestamp").alias("kafka_timestamp")
            ).select("key", "data.*", "kafka_timestamp")

            logger.info("âœ… Kafka stream Ä‘Ã£ Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng")
            return processed_df

        except Exception as e:
            logger.error(f"âŒ Lá»—i khi táº¡o Kafka stream: {e}")
            raise

# ===============================
# Write batch to Postgres - Cáº£i thiá»‡n
# ===============================
def write_to_postgres(batch_df, batch_id):
    try:
        logger.info(f"ğŸ”„ Báº¯t Ä‘áº§u xá»­ lÃ½ Batch {batch_id}")
        
        # Debug: In schema vÃ  sample data
        logger.info(f"ğŸ“Š Schema: {batch_df.schema}")
        
        row_count = batch_df.count()
        logger.info(f"ğŸ“Š Batch {batch_id}: {row_count} records")
        
        if row_count > 0:
            # Hiá»ƒn thá»‹ sample data
            logger.info("ğŸ“‹ Sample data:")
            batch_df.show(5, truncate=False)
            
            # Kiá»ƒm tra data quality
            valid_transactions = batch_df.filter(
                col("transaction_id").isNotNull() &
                col("transaction_amount").isNotNull() &
                (col("transaction_amount") > 0)
            )
            
            valid_count = valid_transactions.count()
            logger.info(f"ğŸ“Š Valid records: {valid_count}/{row_count}")
            
            if valid_count > 0:
                # ThÃªm timestamp
                final_df = valid_transactions.withColumn("created_at", current_timestamp())
                
                logger.info(f"ğŸ’¾ Äang ghi {valid_count} records vÃ o database...")
                
                final_df.write \
                    .format("jdbc") \
                    .option("url", POSTGRES_CONFIG["url"]) \
                    .option("dbtable", POSTGRES_CONFIG["table"]) \
                    .option("user", POSTGRES_CONFIG["user"]) \
                    .option("password", POSTGRES_CONFIG["password"]) \
                    .option("driver", POSTGRES_CONFIG["driver"]) \
                    .option("batchsize", "1000") \
                    .option("isolationLevel", "NONE") \
                    .option("numPartitions", "1") \
                    .mode("append") \
                    .save()

                logger.info(f"âœ… ÄÃ£ ghi thÃ nh cÃ´ng batch {batch_id} - {valid_count} records")
            else:
                logger.warning(f"âš ï¸ Batch {batch_id}: KhÃ´ng cÃ³ dá»¯ liá»‡u há»£p lá»‡")
        else:
            logger.info(f"âš ï¸ Batch {batch_id} trá»‘ng, bá» qua")

    except Exception as e:
        logger.error(f"âŒ Lá»—i khi ghi batch {batch_id}: {e}")
        # Log chi tiáº¿t lá»—i
        import traceback
        logger.error(f"âŒ Stacktrace: {traceback.format_exc()}")

# ===============================
# Main - Cáº£i thiá»‡n
# ===============================
def main():
    try:
        logger.info("ğŸš€ Khá»Ÿi táº¡o Spark Streaming...")
        
        # Táº¡o thÆ° má»¥c checkpoint náº¿u chÆ°a tá»“n táº¡i
        checkpoint_dir = "/user/hadoop/checkpoints/transaction_data"
        logger.info(f"ğŸ“ Checkpoint directory: {checkpoint_dir}")
        
        stream = SparkStreaming(KAFKA_CONFIG, ['transaction_data'])
        df = stream.read_stream()

        # Data quality checks
        df_filtered = df.filter(
            col("transaction_id").isNotNull() &
            col("transaction_amount").isNotNull() &
            (col("transaction_amount") > 0)
        )

        query = df_filtered.writeStream \
            .foreachBatch(write_to_postgres) \
            .outputMode("append") \
            .option("checkpointLocation", checkpoint_dir) \
            .trigger(processingTime="10 seconds") \
            .start()

        logger.info("âœ… Streaming Ä‘Ã£ báº¯t Ä‘áº§u. Nháº¥n Ctrl+C Ä‘á»ƒ dá»«ng...")
        
        # Monitor streaming
        while query.isActive:
            progress = query.lastProgress
            if progress:
                logger.info(f"ğŸ“Š Streaming Progress: {progress}")
            query.awaitTermination(timeout=30)

    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Äang dá»«ng streaming...")
        if 'query' in locals():
            query.stop()
    except Exception as e:
        logger.error(f"âŒ Lá»—i trong main: {e}")
        import traceback
        logger.error(f"âŒ Stacktrace: {traceback.format_exc()}")
        raise

if __name__ == "__main__":
    main()