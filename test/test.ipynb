{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa31043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | Field Name | Data Type | Description (Vietnamese) | Description (English) | Business Rules | Example |\n",
    "# |------------|-----------|--------------------------|----------------------|----------------|---------|\n",
    "# | transaction_key | int | Khóa chính surrogate | Primary surrogate key | PK, NOT NULL, UNIQUE | 1001 |\n",
    "# | customer_key | int | Khóa khách hàng | Customer foreign key | FK to DIM_CUSTOMER | 12345 |\n",
    "# | product_key | int | Khóa sản phẩm | Product foreign key | FK to DIM_PRODUCT | 567 |\n",
    "# | location_key | int | Khóa địa điểm | Location foreign key | FK to DIM_LOCATION | 789 |\n",
    "# | event_key | int | Khóa sự kiện | Event foreign key | FK to DIM_EVENT | 101 |\n",
    "# | involved_party_key | int | Khóa bên liên quan | Involved party foreign key | FK to DIM_INVOLVED_PARTY | 234 |\n",
    "# | condition_key | int | Khóa điều kiện | Condition foreign key | FK to DIM_CONDITION | 345 |\n",
    "# | application_key | int | Khóa ứng dụng | Application foreign key | FK to DIM_APPLICATION | 456 |\n",
    "# | asset_key | int | Khóa tài sản | Asset foreign key | FK to DIM_ASSET | 678 |\n",
    "#------------------------------------------------------------------\n",
    "#    KEY \n",
    "#------------------------------------------------------------------\n",
    "# | transaction_id | string | Mã giao dịch duy nhất | Unique transaction ID | NOT NULL, UNIQUE | TXN20240315143025001 |\n",
    "# | reference_number | string | Số tham chiếu | Reference number | Max 50 chars | REF123456789 |\n",
    "# | transaction_type | string | Loại giao dịch | Transaction type | NOT NULL | Chuyển khoản, Rút tiền, Nạp tiền,  Nhận Tiền |\n",
    "# | transaction_category | string | Danh mục giao dịch | Transaction category | NOT NULL | Nội bộ, Liên ngân hàng, Quốc tế |\n",
    "# | transaction_subcategory | string | Danh mục phụ | Transaction subcategory | Max 100 chars | Chuyển khoản cùng ngân hàng |\n",
    "# | transaction_amount | decimal | Số tiền giao dịch | Transaction amount | NOT NULL, >= 0 | 1000000 |\n",
    "# | fee_amount | decimal | Phí giao dịch | Fee amount | >= 0 | 11000 |\n",
    "# | tax_amount | decimal | Thuế | Tax amount | >= 0 | 1100 |\n",
    "# | net_amount | decimal | Số tiền thực nhận | Net amount | NOT NULL | 987900 |\n",
    "# | currency | string | Loại tiền tệ | Currency code | ISO 4217, NOT NULL | VND |\n",
    "# | transaction_status | string | Trạng thái giao dịch | Transaction status | NOT NULL | Thành công, Thất bại, Đang xử lý |\n",
    "# | channel | string | Kênh giao dịch | Transaction channel | NOT NULL | ATM, Mobile App, Internet Banking |\n",
    "# | description | string | Mô tả giao dịch | Transaction description | Max 500 chars | Chuyển khoản cho thuê nhà |\n",
    "# | created_timestamp | timestamp | Thời gian tạo | Creation timestamp | NOT NULL | 2024-03-15 14:30:25.123 |\n",
    "# | processed_timestamp | timestamp | Thời gian xử lý | Processing timestamp | >= created_timestamp | 2024-03-15 14:30:27.456 |\n",
    "# | updated_timestamp | timestamp | Thời gian cập nhật cuối | Last update timestamp | >= created_timestamp | 2024-03-15 14:31:00.789 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30857e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "539df278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_25712\\1396148292.py:21: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql, conn)\n"
     ]
    },
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got float.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 146\u001b[0m\n\u001b[0;32m    138\u001b[0m conn_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDB_HOST\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDB_PORT\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDB_PASS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m }\n\u001b[0;32m    145\u001b[0m info_transaction \u001b[38;5;241m=\u001b[39m GenerationTranaction(conn_params)\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43minfo_transaction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator_data_transaction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[21], line 96\u001b[0m, in \u001b[0;36mGenerationTranaction.generator_data_transaction\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m transaction_type \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransaction_types)\n\u001b[0;32m     95\u001b[0m transaction_category \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransaction_category)\n\u001b[1;32m---> 96\u001b[0m transaction_amount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m transaction_status \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransaction_status)\n\u001b[0;32m     98\u001b[0m fee_amount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(transaction_amount \u001b[38;5;241m*\u001b[39m random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\utils.py:282\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\functions\\builtin.py:7802\u001b[0m, in \u001b[0;36mround\u001b[1;34m(col, scale)\u001b[0m\n\u001b[0;32m   7800\u001b[0m scale \u001b[38;5;241m=\u001b[39m _enum_to_value(scale)\n\u001b[0;32m   7801\u001b[0m scale \u001b[38;5;241m=\u001b[39m lit(scale) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scale, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m scale\n\u001b[1;32m-> 7802\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function_over_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mround\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\functions\\builtin.py:117\u001b[0m, in \u001b[0;36m_invoke_function_over_columns\u001b[1;34m(name, *cols)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03mInvokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolumn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _to_java_column\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, \u001b[38;5;241m*\u001b[39m(_to_java_column(col) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\functions\\builtin.py:117\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03mInvokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolumn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _to_java_column\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, \u001b[38;5;241m*\u001b[39m(\u001b[43m_to_java_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\classic\\column.py:71\u001b[0m, in \u001b[0;36m_to_java_column\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m     69\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m _create_column_from_name(col)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m     72\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     73\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m     74\u001b[0m     )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jcol\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got float."
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import random\n",
    "import uuid\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class getInfoTransaction:\n",
    "    def __init__(self, conn_params):\n",
    "        self.engine = create_engine(\n",
    "            f\"postgresql+psycopg2://{conn_params['user']}:{conn_params['password']}@{conn_params['host']}:{conn_params['port']}/{conn_params['dbname']}\"\n",
    "        )\n",
    "\n",
    "    def _query(self, sql: str) -> pd.DataFrame:\n",
    "        try:\n",
    "            conn = self.engine.raw_connection()   \n",
    "            try:\n",
    "                df = pd.read_sql_query(sql, conn)\n",
    "            finally:\n",
    "                conn.close()  \n",
    "            return df\n",
    "        except Exception as e:  \n",
    "            print(f\"Error executing query: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def getCustomer(self):\n",
    "        df = self._query(\"SELECT customer_key FROM banking_dw.dim_customer\")\n",
    "        return df\n",
    "\n",
    "    def getLocation(self):\n",
    "        df = self._query(\"SELECT location_key   FROM banking_dw.dim_location\")\n",
    "        return df\n",
    "\n",
    "    def getApplication(self):\n",
    "        df = self._query(\"SELECT application_key FROM banking_dw.dim_application\")\n",
    "        return df\n",
    "\n",
    "    def getAccount(self):\n",
    "        df = self._query(\"SELECT account_key , account_number , customer_key , current_balance FROM banking_dw.dim_account\")\n",
    "        return df\n",
    "\n",
    "class GenerationTranaction :\n",
    "    def __init__(self , conn_params):\n",
    "        self.conn_params = conn_params\n",
    "        self.account = getInfoTransaction(conn_params).getAccount()\n",
    "        self.location = getInfoTransaction(conn_params).getLocation()\n",
    "        self.application = getInfoTransaction(conn_params).getApplication()\n",
    "        self.transaction_types = [\"Chuyển khoản\", \"Rút tiền\", \"Nạp tiền\", \"Nhận Tiền\"]\n",
    "        self.transaction_category = [\"Nội bộ\", \"Liên ngân hàng\", \"Quốc tế\"]\n",
    "        self.transaction_status = [\"Thành công\", \"Thất bại\"]\n",
    "        self.channel = [\"ATM\", \"Mobile App\", \"Internet Banking\"]\n",
    "        \n",
    "    def get_customer_key(self , account_key):\n",
    "        df = self.account[self.account[\"account_key\"] == account_key]\n",
    "        if not df.empty:\n",
    "            return df.iloc[0][\"customer_key\"]\n",
    "        return None\n",
    "    \n",
    "    # tinh tien net nhan duoc khi thuc hien giao dich\n",
    "    def amount(self ,transaction_types , transaction_status,  transaction_amount , fee_amount , tax_amount , ):\n",
    "        if transaction_status == \"Thất bại\":\n",
    "            return 0.0\n",
    "        else:\n",
    "            if transaction_types == \"Rút tiền\":\n",
    "                return round(transaction_amount + fee_amount + tax_amount , 2)\n",
    "            elif transaction_types == \"Nạp tiền\":\n",
    "                return round(transaction_amount)\n",
    "            elif transaction_types == \"Chuyển khoản\":  \n",
    "                return  round(transaction_amount + fee_amount + tax_amount , 2)\n",
    "            elif transaction_types == \"Nhận Tiền\":\n",
    "                return round(transaction_amount)\n",
    "        \n",
    "        \n",
    "    def transaction_account_number(self , transaction_category ):\n",
    "        if transaction_category == \"Nội bộ\":\n",
    "            return random.choice(self.account[\"account_number\"])\n",
    "        elif transaction_category == \"Liên ngân hàng\":\n",
    "            return f\"LB{random.randint(100000, 999999)}\"\n",
    "        elif transaction_category == \"Quốc tế\":\n",
    "            return f\"QT{random.randint(100000, 999999)}\"\n",
    "        \n",
    "    def generator_data_transaction(self):\n",
    "        \n",
    "        account_key   =  int(random.choice(self.account[\"account_key\"]))\n",
    "        customer_key  =  int(self.get_customer_key(account_key))\n",
    "        location_key  =  int(random.choice(self.location[\"location_key\"]))\n",
    "        event_key = uuid.uuid4().int >> 64\n",
    "        application_key = int(random.choice(self.account[\"account_key\"]))\n",
    "        transaction_id = f\"TXN{datetime.now().strftime('%Y%m%d%H%M%S')}{random.randint(100, 999)}\"\n",
    "        reference_number = f\"REF{uuid.uuid4().hex[:10].upper()}\"\n",
    "        transaction_type = random.choice(self.transaction_types)\n",
    "        transaction_category = random.choice(self.transaction_category)\n",
    "        transaction_amount = round(random.uniform(10000, 10000000), 2)\n",
    "        transaction_status = random.choice(self.transaction_status)\n",
    "        fee_amount = round(transaction_amount * random.uniform(0.001, 0.01), 2)\n",
    "        tax_amount = round(fee_amount * 0.1, 2)\n",
    "        net_amount = self.amount(transaction_type,transaction_status,transaction_amount, fee_amount, tax_amount) \n",
    "        currency = \"VND\"\n",
    "        account_number = self.transaction_account_number(transaction_category)\n",
    "\n",
    "        channel = random.choice(self.channel)\n",
    "        description = f\"Giao dịch {transaction_type} qua {channel}\"\n",
    "        created_timestamp = datetime.now()\n",
    "        processed_timestamp = created_timestamp + timedelta(seconds=random.randint(1, 300))\n",
    "        updated_timestamp = processed_timestamp + timedelta(seconds=random.randint(1, 300))\n",
    "\n",
    "        \n",
    "        return {\n",
    "            \"account_key\": account_key,\n",
    "            \"customer_key\": customer_key,\n",
    "            \"location_key\": location_key,\n",
    "            \"event_key\": event_key,\n",
    "            \"application_key\": application_key,\n",
    "            \"transaction_id\": transaction_id,\n",
    "            \"reference_number\": reference_number,\n",
    "            \"transaction_type\": transaction_type,\n",
    "            \"transaction_category\": transaction_category,\n",
    "            \"transaction_amount\": transaction_amount,\n",
    "            \"fee_amount\": fee_amount,\n",
    "            \"tax_amount\": tax_amount,\n",
    "            \"net_amount\": net_amount,\n",
    "            \"currency\": currency,\n",
    "            \"account_number\": account_number,\n",
    "            \"transaction_status\": transaction_status,\n",
    "            \"channel\": channel,\n",
    "            \"description\": description,\n",
    "            \"created_timestamp\": created_timestamp,\n",
    "            \"processed_timestamp\": processed_timestamp,\n",
    "            \"updated_timestamp\": updated_timestamp\n",
    "        }\n",
    "          \n",
    "if __name__ == \"__main__\":  \n",
    "    load_dotenv()\n",
    "    \n",
    "    conn_params = {\n",
    "        \"host\": os.getenv(\"DB_HOST\"),\n",
    "        \"port\": os.getenv(\"DB_PORT\"),\n",
    "        \"dbname\": os.getenv(\"DB_NAME\"),\n",
    "        \"user\": os.getenv(\"DB_USER\"),\n",
    "        \"password\": os.getenv(\"DB_PASS\")\n",
    "    }\n",
    "    info_transaction = GenerationTranaction(conn_params)\n",
    "    print(info_transaction.generator_data_transaction())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09fd2233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 14:00:53,611 - __main__ - INFO - 🔧 Database: jdbc:postgresql://192.168.235.136:5432/dwh\n",
      "2025-09-09 14:00:53,612 - __main__ - INFO - 🔧 Table: banking_dw.fact_transaction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 14:00:53,614 - __main__ - INFO - 🚀 STARTING REALTIME KAFKA TO POSTGRES STREAMING\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 184\u001b[0m\n\u001b[0;32m    181\u001b[0m     realtime_stream\u001b[38;5;241m.\u001b[39mstart_realtime_processing()\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 184\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 180\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    179\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 STARTING REALTIME KAFKA TO POSTGRES STREAMING\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 180\u001b[0m     realtime_stream \u001b[38;5;241m=\u001b[39m \u001b[43mRealtimeSparkStreaming\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m     realtime_stream\u001b[38;5;241m.\u001b[39mstart_realtime_processing()\n",
      "Cell \u001b[1;32mIn[20], line 122\u001b[0m, in \u001b[0;36mRealtimeSparkStreaming.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRealtimeKafkaToPostgres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.cores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.shuffle.partitions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.adaptive.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.serializer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.serializer.KryoSerializer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.execution.arrow.pyspark.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.streaming.forceDeleteTempCheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars.packages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2,org.postgresql:postgresql:42.2.18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚡ Spark session created for realtime processing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\session.py:559\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    556\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m--> 559\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    561\u001b[0m     module \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_get_j_spark_session_module(session\u001b[38;5;241m.\u001b[39m_jvm)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\session.py:635\u001b[0m, in \u001b[0;36mSparkSession.__init__\u001b[1;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[0;32m    631\u001b[0m jSparkSessionModule \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_get_j_spark_session_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jsparkSession \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m--> 635\u001b[0m         \u001b[43mjSparkSessionClass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDefaultSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39misDefined()\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jSparkSessionClass\u001b[38;5;241m.\u001b[39mgetDefaultSession()\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39msparkContext()\u001b[38;5;241m.\u001b[39misStopped()\n\u001b[0;32m    637\u001b[0m     ):\n\u001b[0;32m    638\u001b[0m         jsparkSession \u001b[38;5;241m=\u001b[39m jSparkSessionClass\u001b[38;5;241m.\u001b[39mgetDefaultSession()\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m    639\u001b[0m         jSparkSessionModule\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(jsparkSession, options)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "# ===============================\n",
    "# Logging setup\n",
    "# ===============================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables (.env đặt ngoài project/consumer)\n",
    "load_dotenv(dotenv_path=os.path.join(\"/home/hadoop/project\", \".env\"))\n",
    "\n",
    "# ===============================\n",
    "# Config\n",
    "# ===============================\n",
    "KAFKA_CONFIG = {\n",
    "    'bootstrap.servers': '192.168.235.136:9092,192.168.235.147:9092,192.168.235.148:9092',\n",
    "}\n",
    "\n",
    "POSTGRES_CONFIG = {\n",
    "    \"url\": f\"jdbc:postgresql://{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\",\n",
    "    \"table\": \"banking_dw.fact_transaction\",\n",
    "    \"user\": os.getenv(\"DB_USER\"),\n",
    "    \"password\": os.getenv(\"DB_PASS\"),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "logger.info(f\"🔧 Database: {POSTGRES_CONFIG['url']}\")\n",
    "logger.info(f\"🔧 Table: {POSTGRES_CONFIG['table']}\")\n",
    "\n",
    "# ===============================\n",
    "# Schema khớp với bảng fact_transaction\n",
    "# ===============================\n",
    "TRANSACTION_SCHEMA = StructType([\n",
    "    StructField(\"account_key\", IntegerType(), True),\n",
    "    StructField(\"customer_key\", IntegerType(), True),\n",
    "    StructField(\"location_key\", IntegerType(), True),\n",
    "    StructField(\"event_key\", IntegerType(), True),\n",
    "    StructField(\"application_key\", IntegerType(), True),\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"reference_number\", StringType(), True),\n",
    "    StructField(\"transaction_type\", StringType(), True),\n",
    "    StructField(\"transaction_category\", StringType(), True),\n",
    "    StructField(\"transaction_amount\", DoubleType(), True),\n",
    "    StructField(\"transaction_status\", StringType(), True),\n",
    "    StructField(\"fee_amount\", DoubleType(), True),\n",
    "    StructField(\"tax_amount\", DoubleType(), True),\n",
    "    StructField(\"net_amount\", DoubleType(), True),\n",
    "    StructField(\"currency\", StringType(), True),\n",
    "    StructField(\"account_number\", StringType(), True),\n",
    "    StructField(\"channel\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"created_timestamp\", TimestampType(), True),\n",
    "    StructField(\"processed_timestamp\", TimestampType(), True),\n",
    "    StructField(\"updated_timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# ===============================\n",
    "# Realtime Write Function\n",
    "# ===============================\n",
    "def write_realtime_batch(batch_df, batch_id):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        row_count = batch_df.count()\n",
    "        logger.info(f\"⚡ Batch {batch_id}: Processing {row_count} records\")\n",
    "\n",
    "        if row_count == 0:\n",
    "            return\n",
    "\n",
    "        # Validate: transaction_id + transaction_amount phải có\n",
    "        valid_df = batch_df.filter(\n",
    "            col(\"transaction_id\").isNotNull() &\n",
    "            col(\"transaction_amount\").isNotNull() &\n",
    "            (col(\"transaction_amount\") > 0)\n",
    "        )\n",
    "\n",
    "        valid_count = valid_df.count()\n",
    "        if valid_count == 0:\n",
    "            logger.warning(f\"⚡ Batch {batch_id}: No valid records\")\n",
    "            return\n",
    "\n",
    "        # Add processing timestamp\n",
    "        final_df = valid_df.withColumn(\"realtime_processed_at\", current_timestamp())\n",
    "\n",
    "        # Write to PostgreSQL\n",
    "        final_df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", POSTGRES_CONFIG[\"url\"]) \\\n",
    "            .option(\"dbtable\", POSTGRES_CONFIG[\"table\"]) \\\n",
    "            .option(\"user\", POSTGRES_CONFIG[\"user\"]) \\\n",
    "            .option(\"password\", POSTGRES_CONFIG[\"password\"]) \\\n",
    "            .option(\"driver\", POSTGRES_CONFIG[\"driver\"]) \\\n",
    "            .option(\"batchsize\", \"500\") \\\n",
    "            .option(\"isolationLevel\", \"READ_UNCOMMITTED\") \\\n",
    "            .option(\"numPartitions\", \"1\") \\\n",
    "            .option(\"rewriteBatchedStatements\", \"true\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "        logger.info(f\"✅ Batch {batch_id}: Inserted {valid_count} records in {processing_time:.2f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        processing_time = time.time() - start_time\n",
    "        logger.error(f\"❌ Batch {batch_id} failed after {processing_time:.2f}s: {e}\")\n",
    "        logger.error(f\"❌ Error details: {traceback.format_exc()}\")\n",
    "\n",
    "# ===============================\n",
    "# Realtime Streaming Class\n",
    "# ===============================\n",
    "class RealtimeSparkStreaming:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"RealtimeKafkaToPostgres\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .config(\"spark.executor.memory\", \"2g\") \\\n",
    "            .config(\"spark.executor.cores\", \"2\") \\\n",
    "            .config(\"spark.driver.memory\", \"1g\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "            .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2,org.postgresql:postgresql:42.2.18\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "        logger.info(\"⚡ Spark session created for realtime processing\")\n",
    "\n",
    "    def create_realtime_stream(self):\n",
    "        logger.info(\"⚡ Creating realtime Kafka stream...\")\n",
    "\n",
    "        kafka_df = self.spark.readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", KAFKA_CONFIG['bootstrap.servers']) \\\n",
    "            .option(\"subscribe\", \"transaction_data\") \\\n",
    "            .option(\"startingOffsets\", \"latest\") \\\n",
    "            .option(\"failOnDataLoss\", \"false\") \\\n",
    "            .option(\"maxOffsetsPerTrigger\", \"1000\") \\\n",
    "            .option(\"kafka.consumer.cache.enabled\", \"false\") \\\n",
    "            .load()\n",
    "\n",
    "        parsed_df = kafka_df.select(\n",
    "            col(\"key\").cast(\"string\"),\n",
    "            from_json(col(\"value\").cast(\"string\"), TRANSACTION_SCHEMA).alias(\"data\"),\n",
    "            col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    "        ).select(\"key\", \"data.*\", \"kafka_timestamp\")\n",
    "\n",
    "        parsed_df.printSchema()\n",
    "        logger.info(\"✅ Realtime Kafka stream created successfully\")\n",
    "        return parsed_df\n",
    "\n",
    "    def start_realtime_processing(self):\n",
    "        df = self.create_realtime_stream()\n",
    "\n",
    "        query = df.writeStream \\\n",
    "            .foreachBatch(write_realtime_batch) \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .option(\"checkpointLocation\", \"/tmp/realtime_checkpoint\") \\\n",
    "            .trigger(processingTime=\"2 seconds\") \\\n",
    "            .start()\n",
    "\n",
    "        logger.info(\"⚡ REALTIME PROCESSING STARTED!\")\n",
    "        query.awaitTermination()\n",
    "\n",
    "# ===============================\n",
    "# Main Function\n",
    "# ===============================\n",
    "def main():\n",
    "    logger.info(\"🚀 STARTING REALTIME KAFKA TO POSTGRES STREAMING\")\n",
    "    realtime_stream = RealtimeSparkStreaming()\n",
    "    realtime_stream.start_realtime_processing()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a00e5ed4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m----> 3\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKafkaConsumerTest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mreadStream \\\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m192.168.235.136:9092\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransaction_data\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m     14\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\session.py:559\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    556\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m--> 559\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    561\u001b[0m     module \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_get_j_spark_session_module(session\u001b[38;5;241m.\u001b[39m_jvm)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\session.py:635\u001b[0m, in \u001b[0;36mSparkSession.__init__\u001b[1;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[0;32m    631\u001b[0m jSparkSessionModule \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_get_j_spark_session_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jsparkSession \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m--> 635\u001b[0m         \u001b[43mjSparkSessionClass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDefaultSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39misDefined()\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jSparkSessionClass\u001b[38;5;241m.\u001b[39mgetDefaultSession()\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39msparkContext()\u001b[38;5;241m.\u001b[39misStopped()\n\u001b[0;32m    637\u001b[0m     ):\n\u001b[0;32m    638\u001b[0m         jsparkSession \u001b[38;5;241m=\u001b[39m jSparkSessionClass\u001b[38;5;241m.\u001b[39mgetDefaultSession()\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m    639\u001b[0m         jSparkSessionModule\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(jsparkSession, options)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaConsumerTest\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"192.168.235.136:9092\") \\\n",
    "    .option(\"subscribe\", \"transaction_data\") \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
